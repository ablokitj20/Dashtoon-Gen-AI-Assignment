# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcV8NwzJwFSvS2iK4Rirk8E2pF8Wwwts

Import Necessary Libraries
"""

import torch
from torch.optim import Adam, LBFGS
from torch.autograd import Variable
import numpy as np
import os
import argparse
import cv2 as cv
import numpy as np
from torchvision import transforms
import matplotlib.pyplot as plt
import torch.nn as nn
import torchvision.models as models
from collections import namedtuple
from matplotlib import pyplot as plt


device = torch.device("cuda" if torch.cuda.is_available() else "cpu") #Setting up the GPU if available, else CPU
print(f"Using {device}")
ImageNet_Means = [123.68,116.779,103.939] #to subtract these mean values from the corresponding RGB channel
ImageNet_STD = [1,1,1] #Std deviation used for normalizing, and we want no scaling to mainatain original range of pixel value


def preprocess_img(img_path, target_shape, device = device):
  img = cv.imread(img_path)[:, :, ::-1] # converting BGR to RGB
  if type(target_shape) == tuple:
    img = cv.resize(img, (target_shape[1], target_shape[0]), interpolation=cv.INTER_CUBIC)
  else:
    current_height, current_width = img.shape[:2]
    new_height = target_shape
    ratio = new_height / current_height
    new_width = int(current_width * ratio)
    img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_CUBIC)

  # normalizing using ImageNet's mean
  transform = transforms.Compose([
      transforms.ToTensor(),
      transforms.Lambda(lambda x: x.mul(255)),
      transforms.Normalize(mean=ImageNet_Means, std=ImageNet_STD)
  ])
  img = transform(img).to(device).unsqueeze(0)
  return img #-> torch.tensor


class Vgg19(torch.nn.Module):

    def __init__(self, requires_grad=False, show_progress=False, use_relu=True):
        super().__init__()
        vgg_pretrained_features = models.vgg19(pretrained=True, progress=show_progress).features
        if use_relu:  # use relu or as in original paper conv layers
            self.layer_names = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'conv4_2', 'relu5_1']
            self.offset = 1
        else:
            self.layer_names = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv4_2', 'conv5_1']
            self.offset = 0
        self.content_feature_maps_index = 4  # conv4_2
        # all layers used for style representation except conv4_2
        self.style_feature_maps_indices = list(range(len(self.layer_names)))
        self.style_feature_maps_indices.remove(4)  # conv4_2

        self.slice1 = torch.nn.Sequential()
        self.slice2 = torch.nn.Sequential()
        self.slice3 = torch.nn.Sequential()
        self.slice4 = torch.nn.Sequential()
        self.slice5 = torch.nn.Sequential()
        self.slice6 = torch.nn.Sequential()
        for x in range(1+self.offset):
            self.slice1.add_module(str(x), vgg_pretrained_features[x])
        for x in range(1+self.offset, 6+self.offset):
            self.slice2.add_module(str(x), vgg_pretrained_features[x])
        for x in range(6+self.offset, 11+self.offset):
            self.slice3.add_module(str(x), vgg_pretrained_features[x])
        for x in range(11+self.offset, 20+self.offset):
            self.slice4.add_module(str(x), vgg_pretrained_features[x])
        for x in range(20+self.offset, 22):
            self.slice5.add_module(str(x), vgg_pretrained_features[x])
        for x in range(22, 29++self.offset):
            self.slice6.add_module(str(x), vgg_pretrained_features[x])
        if not requires_grad:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, x):
        x = self.slice1(x)
        layer1_1 = x
        x = self.slice2(x)
        layer2_1 = x
        x = self.slice3(x)
        layer3_1 = x
        x = self.slice4(x)
        layer4_1 = x
        x = self.slice5(x)
        conv4_2 = x
        x = self.slice6(x)
        layer5_1 = x
        vgg_outputs = namedtuple("VggOutputs", self.layer_names)
        out = vgg_outputs(layer1_1, layer2_1, layer3_1, layer4_1, conv4_2, layer5_1)
        return out


def prepare_model( device = device):
  #Create an instance of the Vgg19 model
  model = Vgg19(requires_grad=False, show_progress=True)
  # Extract relevant information about content and style feature maps
  content_feature_maps_index = model.content_feature_maps_index
  style_feature_maps_indices = model.style_feature_maps_indices
  layer_names = model.layer_names
  # Create tuples with feature maps information
  content_fms_index_name = (content_feature_maps_index, layer_names[content_feature_maps_index])
  style_fms_indices_names = (style_feature_maps_indices, layer_names)
  # Move the model to the specified device, set to evaluation mode, and return
  return model.to(device).eval(), content_fms_index_name, style_fms_indices_names


def gram_matrix(x, should_normalize=True):
    (b, ch, h, w) = x.size()
    features = x.view(b, ch, w * h)
    features_t = features.transpose(1, 2)
    gram = features.bmm(features_t)
    if should_normalize:
        gram /= ch * h * w
    return gram

def Loss_Functions(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config):
    target_content_representation = target_representations[0]
    target_style_representation = target_representations[1]

    current_set_of_feature_maps = neural_net(optimizing_img)

    current_content_representation = current_set_of_feature_maps[content_feature_maps_index].squeeze(axis=0)
    content_loss = torch.nn.MSELoss(reduction='mean')(target_content_representation, current_content_representation)

    style_loss = 0.0
    current_style_representation = [gram_matrix(x) for cnt, x in enumerate(current_set_of_feature_maps) if cnt in style_feature_maps_indices]
    for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):
        style_loss += torch.nn.MSELoss(reduction='sum')(gram_gt[0], gram_hat[0])
    style_loss /= len(target_style_representation)

    tv_loss = torch.sum(torch.abs(optimizing_img[:, :, :, :-1] - optimizing_img[:, :, :, 1:])) + torch.sum(torch.abs(optimizing_img[:, :, :-1, :] - optimizing_img[:, :, 1:, :]))

    total_loss = config['content_weight'] * content_loss + config['style_weight'] * style_loss + config['tv_weight'] * tv_loss

    return total_loss, content_loss, style_loss, tv_loss

def make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index, style_feature_maps_indices, config):
    def tuning_step(optimizing_img):
        # Calculate loss
        total_loss, content_loss, style_loss, tv_loss = build_loss(
            neural_net, optimizing_img, target_representations,
            content_feature_maps_index, style_feature_maps_indices, config
        )
        # Backpropagate gradients
        total_loss.backward()
        # Update model parameters
        optimizer.step()
        # Zero out gradients for the next iteration
        optimizer.zero_grad()
        return total_loss, content_loss, style_loss, tv_loss
    # Return the tuning step function
    return tuning_step

def save_image(optimizing_img, dump_path, config, cnt, num_of_iterations):
    out_img = optimizing_img.squeeze(axis=0).to('cpu').detach().numpy()
    out_img = np.moveaxis(out_img, 0, 2)  # swap channel from 1st to 3rd position: ch, _, _ -> _, _, chr

    if cnt == num_of_iterations - 1:
        out_img_name = "Styled_Image.jpg"
        current_dir = os.getcwd()
        data_dir = os.path.join(current_dir, 'data')
        dump_img = np.copy(out_img)
        dump_img += np.array(ImageNet_Means).reshape((1, 1, 3))
        dump_img = np.clip(dump_img, 0, 255).astype('uint8')
        cv.imwrite(os.path.join(data_dir, out_img_name), dump_img[:, :, ::-1])


def neural_style_transfer(config, save_model_path=None):
    content_img_path = config['content_img_path']
    style_img_path = config['style_img_path']
    out_dir_name = 'combined_' + os.path.split(content_img_path)[1].split('.')[0] + '_' + os.path.split(style_img_path)[1].split('.')[0]
    dump_path = os.path.join(config['output_img_dir'], out_dir_name)
    os.makedirs(dump_path, exist_ok=True)

    content_img = preprocess_img(content_img_path, config['height'], device)
    style_img = preprocess_img(style_img_path, config['height'], device)

    optimizing_img = Variable(content_img, requires_grad=True)
    neural_net, content_feature_maps_index_name, style_feature_maps_indices_names = prepare_model()

    content_img_set_of_feature_maps = neural_net(content_img)
    style_img_set_of_feature_maps = neural_net(style_img)

    target_content_representation = content_img_set_of_feature_maps[content_feature_maps_index_name[0]].squeeze(axis=0)
    target_style_representation = [gram_matrix(x) for cnt, x in enumerate(style_img_set_of_feature_maps) if cnt in style_feature_maps_indices_names[0]]
    target_representations = [target_content_representation, target_style_representation]

    num_of_iterations = {
        "lbfgs": 150,
        "adam": 2000,
    }

    if config['optimizer'] == 'adam':
        optimizer = Adam((optimizing_img,), lr=1e1)
        tuning_step = make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)

        for cnt in range(num_of_iterations[config['optimizer']]):
            total_loss, content_loss, style_loss, tv_loss = tuning_step(optimizing_img)
            with torch.no_grad():
                print(f'Adam | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config["content_weight"] * content_loss.item():12.4f}, style loss={config["style_weight"] * style_loss.item():12.4f}, tv loss={config["tv_weight"] * tv_loss.item():12.4f}')
                save_image(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']])

    elif config['optimizer'] == 'lbfgs':
        optimizer = LBFGS((optimizing_img,), max_iter=num_of_iterations['lbfgs'], line_search_fn='strong_wolfe')
        cnt = 0

        def closure():
            nonlocal cnt
            if torch.is_grad_enabled():
                optimizer.zero_grad()

            total_loss, content_loss, style_loss, tv_loss = Loss_Functions(neural_net, optimizing_img, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)
            if total_loss.requires_grad:
                total_loss.backward()

            with torch.no_grad():
                print(f'L-BFGS | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config["content_weight"] * content_loss.item():12.4f}, style loss={config["style_weight"] * style_loss.item():12.4f}, tv loss={config["tv_weight"] * tv_loss.item():12.4f}')
                save_image(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']])
            cnt += 1
            return total_loss

        optimizer.step(closure)
    return dump_path

content_img_path = input("Enter you content image directory:")
style_img_path =  input("Enter you style image directory:")
target_shape = 400
current_dir = os.getcwd()
default_resource_dir = os.path.join(current_dir, 'data')
model_dir = os.path.join(default_resource_dir, 'Saved Model')
config = {
    'height': target_shape,
    'content_weight': 1,
    'style_weight': .5,
    'tv_weight': 1e-3,
    'optimizer': 'lbfgs',
    'content_img_path': content_img_path,
    'style_img_path': style_img_path,
    'output_img_dir': default_resource_dir,
    'img_format': (3,'.jpg')
}
results_path = neural_style_transfer(config, model_dir)